{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\allan\\anaconda3\\lib\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary library\n",
    "################################################\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# all lightfm imports \n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm import cross_validation\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import auc_score\n",
    "\n",
    "# imports re for text cleaning \n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# we will ignore pandas warning \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all our datasets and store them in pandas dataframe objects. \n",
    "############################################\n",
    "base_path = 'Data/'\n",
    "df_answer_scores = pd.read_csv(\n",
    "    base_path + 'answer_scores.csv')\n",
    "\n",
    "df_answers = pd.read_csv(\n",
    "    base_path + 'answers.csv',\n",
    "    parse_dates=['answers_date_added'])\n",
    "\n",
    "df_comments = pd.read_csv(\n",
    "    base_path + 'comments.csv')\n",
    "\n",
    "df_emails = pd.read_csv(\n",
    "    base_path + 'emails.csv')\n",
    "\n",
    "df_group_memberships = pd.read_csv(\n",
    "    base_path + 'group_memberships.csv')\n",
    "\n",
    "df_groups = pd.read_csv(\n",
    "    base_path + 'groups.csv')\n",
    "\n",
    "df_matches = pd.read_csv(\n",
    "    base_path + 'matches.csv')\n",
    "\n",
    "df_professionals = pd.read_csv(\n",
    "    base_path + 'professionals.csv',\n",
    "    parse_dates=['professionals_date_joined'])\n",
    "\n",
    "df_question_scores = pd.read_csv(\n",
    "    base_path + 'question_scores.csv')\n",
    "\n",
    "df_questions = pd.read_csv(\n",
    "    base_path + 'questions.csv',\n",
    "    parse_dates=['questions_date_added'])\n",
    "\n",
    "df_school_memberships = pd.read_csv(\n",
    "    base_path + 'school_memberships.csv')\n",
    "\n",
    "df_students = pd.read_csv(\n",
    "    base_path + 'students.csv',\n",
    "    parse_dates=['students_date_joined'])\n",
    "\n",
    "df_tag_questions = pd.read_csv(\n",
    "    base_path + 'tag_questions.csv')\n",
    "\n",
    "df_tag_users = pd.read_csv(\n",
    "    base_path + 'tag_users.csv')\n",
    "\n",
    "df_tags = pd.read_csv(\n",
    "    base_path + 'tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_int_id(dataframe, id_col_name):\n",
    "    \"\"\"\n",
    "    Generate unique integer id for users, questions and answers\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Pandas Dataframe for Users or Q&A. \n",
    "    id_col_name : String \n",
    "        New integer id's column name.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Dataframe\n",
    "        Updated dataframe containing new id column \n",
    "    \"\"\"\n",
    "    new_dataframe=dataframe.assign(\n",
    "        int_id_col_name=np.arange(len(dataframe))\n",
    "        ).reset_index(drop=True)\n",
    "    return new_dataframe.rename(columns={'int_id_col_name': id_col_name})\n",
    "\n",
    "\n",
    "\n",
    "def create_features(dataframe, features_name, id_col_name):\n",
    "    \"\"\"\n",
    "    Generate features that will be ready for feeding into lightfm\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Pandas Dataframe which contains features\n",
    "    features_name : List\n",
    "        List of feature columns name avaiable in dataframe\n",
    "    id_col_name: String\n",
    "        Column name which contains id of the question or\n",
    "        answer that the features will map to.\n",
    "        There are two possible values for this variable.\n",
    "        1. questions_id_num\n",
    "        2. professionals_id_num\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Pandas Series\n",
    "        A pandas series containing process features\n",
    "        that are ready for feed into lightfm.\n",
    "        The format of each value\n",
    "        will be (user_id, ['feature_1', 'feature_2', 'feature_3'])\n",
    "        Ex. -> (1, ['military', 'army', '5'])\n",
    "    \"\"\"\n",
    "\n",
    "    features = dataframe[features_name].apply(\n",
    "        lambda x: ','.join(x.map(str)), axis=1)\n",
    "    features = features.str.split(',')\n",
    "    features = list(zip(dataframe[id_col_name], features))\n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def generate_feature_list(dataframe, features_name):\n",
    "    \"\"\"\n",
    "    Generate features list for mapping \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataframe: Dataframe\n",
    "        Pandas Dataframe for Users or Q&A. \n",
    "    features_name : List\n",
    "        List of feature columns name avaiable in dataframe. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of all features for mapping \n",
    "    \"\"\"\n",
    "    features = dataframe[features_name].apply(\n",
    "        lambda x: ','.join(x.map(str)), axis=1)\n",
    "    features = features.str.split(',')\n",
    "    features = features.apply(pd.Series).stack().reset_index(drop=True)\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_auc_score(lightfm_model, interactions_matrix, \n",
    "                        question_features, professional_features): \n",
    "    \"\"\"\n",
    "    Measure the ROC AUC metric for a model. \n",
    "    A perfect score is 1.0.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lightfm_model: LightFM model \n",
    "        A fitted lightfm model \n",
    "    interactions_matrix : \n",
    "        A lightfm interactions matrix \n",
    "    question_features, professional_features: \n",
    "        Lightfm features \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    String containing AUC score \n",
    "    \"\"\"\n",
    "    score = auc_score( \n",
    "        lightfm_model, interactions_matrix, \n",
    "        item_features=question_features, \n",
    "        user_features=professional_features, \n",
    "        num_threads=4).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating unique integer id for users and q&a\n",
    "df_professionals = generate_int_id(df_professionals, 'professionals_id_num')\n",
    "df_students = generate_int_id(df_students, 'students_id_num')\n",
    "df_questions = generate_int_id(df_questions, 'questions_id_num')\n",
    "df_answers = generate_int_id(df_answers, 'answers_id_num')\n",
    "\n",
    "#df_answers.groupby(['answers_author_id'], sort=False).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging dataset\n",
    "###########################\n",
    "\n",
    "# just dropna from tags \n",
    "df_tags = df_tags.dropna()\n",
    "df_tags['tags_tag_name'] = df_tags['tags_tag_name'].str.replace('#', '')\n",
    "\n",
    "\n",
    "# merge tag_questions with tags name\n",
    "# then group all tags for each question into single rows\n",
    "df_tags_question = df_tag_questions.merge(\n",
    "    df_tags, how='inner',\n",
    "    left_on='tag_questions_tag_id', right_on='tags_tag_id')\n",
    "df_tags_question = df_tags_question.groupby(\n",
    "    ['tag_questions_question_id'])['tags_tag_name'].apply(\n",
    "        ','.join).reset_index()\n",
    "df_tags_question = df_tags_question.rename(columns={'tags_tag_name': 'questions_tag_name'})\n",
    "\n",
    "# merge tag_users with tags name \n",
    "# then group all tags for each user into single rows \n",
    "# after that rename the tag column name \n",
    "df_tags_pro = df_tag_users.merge(\n",
    "    df_tags, how='inner',\n",
    "    left_on='tag_users_tag_id', right_on='tags_tag_id')\n",
    "df_tags_pro = df_tags_pro.groupby(\n",
    "    ['tag_users_user_id'])['tags_tag_name'].apply(\n",
    "        ','.join).reset_index()\n",
    "df_tags_pro = df_tags_pro.rename(columns={'tags_tag_name': 'professionals_tag_name'})\n",
    "\n",
    "\n",
    "# merge professionals and questions tags with main merge_dataset \n",
    "df_questions = df_questions.merge(\n",
    "    df_tags_question, how='left',\n",
    "    left_on='questions_id', right_on='tag_questions_question_id')\n",
    "df_professionals = df_professionals.merge(\n",
    "    df_tags_pro, how='left',\n",
    "    left_on='professionals_id', right_on='tag_users_user_id')\n",
    "\n",
    "# merge questions with scores \n",
    "df_questions = df_questions.merge(\n",
    "    df_question_scores, how='left',\n",
    "    left_on='questions_id', right_on='id')\n",
    "# merge questions with students \n",
    "df_questions = df_questions.merge(\n",
    "    df_students, how='left',\n",
    "    left_on='questions_author_id', right_on='students_id')\n",
    "\n",
    "\n",
    "\n",
    "# merge answers with questions \n",
    "# then merge professionals and questions score with that \n",
    "df_merge = df_answers.merge(\n",
    "    df_questions, how='inner',\n",
    "    left_on='answers_question_id', right_on='questions_id')\n",
    "df_merge = df_merge.merge(\n",
    "    df_professionals, how='inner',\n",
    "    left_on='answers_author_id', right_on='professionals_id')\n",
    "df_merge = df_merge.merge(\n",
    "    df_question_scores, how='inner',\n",
    "    left_on='questions_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some features for calculates weights\n",
    "# that will use with interaction matrix \n",
    "#######################\n",
    "\n",
    "df_merge['num_of_ans_by_professional'] = df_merge.groupby(['answers_author_id'])['questions_id'].transform('count')\n",
    "df_merge['num_ans_per_ques'] = df_merge.groupby(['questions_id'])['answers_id'].transform('count')\n",
    "df_merge['num_tags_professional'] = df_merge['professionals_tag_name'].str.split(\",\").str.len()\n",
    "df_merge['num_tags_question'] = df_merge['questions_tag_name'].str.split(\",\").str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of answer per question : 58\n",
      "Maximum number of tags per professional : 82.0\n",
      "Maximum number of tags per question : 54.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum number of answer per question : \" + str(df_merge['num_ans_per_ques'].max()))\n",
    "print(\"Maximum number of tags per professional : \" + str(df_merge['num_tags_professional'].max()))\n",
    "print(\"Maximum number of tags per question : \" + str(df_merge['num_tags_question'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge professionals previous answered \n",
    "# questions tags into professionals tags \n",
    "########################\n",
    "\n",
    "# select professionals answered questions tags \n",
    "# and stored as a dataframe\n",
    "professionals_prev_ans_tags = df_merge[['professionals_id', 'questions_tag_name']]\n",
    "# drop null values from that \n",
    "professionals_prev_ans_tags = professionals_prev_ans_tags.dropna()\n",
    "# because professsionals answers multiple questions, \n",
    "# we group all of tags of each user into single row \n",
    "professionals_prev_ans_tags = professionals_prev_ans_tags.groupby(\n",
    "    ['professionals_id'])['questions_tag_name'].apply(\n",
    "        ','.join).reset_index()\n",
    "\n",
    "# drop duplicates tags from each professionals rows\n",
    "professionals_prev_ans_tags['questions_tag_name'] = (\n",
    "    professionals_prev_ans_tags['questions_tag_name'].str.split(',').apply(set).str.join(','))\n",
    "\n",
    "# finally merge the dataframe with professionals dataframe \n",
    "df_professionals = df_professionals.merge(professionals_prev_ans_tags, how='left', on='professionals_id')\n",
    "\n",
    "# join professionals tags and their answered tags \n",
    "# we replace nan values with \"\"\n",
    "df_professionals['professional_all_tags'] = (\n",
    "    df_professionals[['professionals_tag_name', 'questions_tag_name']].apply(\n",
    "        lambda x: ','.join(x.dropna()),\n",
    "        axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling null values \n",
    "df_questions['score'] = df_questions['score'].fillna(0)\n",
    "df_questions['score'] = df_questions['score'].astype(int)\n",
    "df_questions['questions_tag_name'] = df_questions['questions_tag_name'].fillna('No Tag')\n",
    "# remove duplicates tags from each questions \n",
    "df_questions['questions_tag_name'] = df_questions['questions_tag_name'].str.split(',').apply(set).str.join(',')\n",
    "\n",
    "\n",
    "# fill nan with 'No Tag' if any \n",
    "df_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].fillna('No Tag')\n",
    "# replace \"\" with \"No Tag\", because previously we replace nan with \"\"\n",
    "df_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].replace('', 'No Tag')\n",
    "df_professionals['professionals_location'] = df_professionals['professionals_location'].fillna('No Location')\n",
    "df_professionals['professionals_industry'] = df_professionals['professionals_industry'].fillna('No Industry')\n",
    "\n",
    "# remove duplicates tags from each professionals \n",
    "df_professionals['professional_all_tags'] = df_professionals['professional_all_tags'].str.split(',').apply(set).str.join(',')\n",
    "\n",
    "\n",
    "\n",
    "# remove some null values from df_merge\n",
    "df_merge['num_ans_per_ques']  = df_merge['num_ans_per_ques'].fillna(0)\n",
    "df_merge['num_tags_professional'] = df_merge['num_tags_professional'].fillna(0)\n",
    "df_merge['num_tags_question'] = df_merge['num_tags_question'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating features list for mapping \n",
    "question_feature_list = generate_feature_list(\n",
    "    df_questions,\n",
    "    ['questions_tag_name'])\n",
    "\n",
    "professional_feature_list = generate_feature_list(\n",
    "    df_professionals,\n",
    "    ['professional_all_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate our weight value \n",
    "df_merge['total_weights'] = 1 / (\n",
    "    df_merge['num_ans_per_ques'])\n",
    "\n",
    "\n",
    "# creating features for feeding into lightfm \n",
    "df_questions['question_features'] = create_features(\n",
    "    df_questions, ['questions_tag_name'], \n",
    "    'questions_id_num')\n",
    "\n",
    "df_professionals['professional_features'] = create_features(\n",
    "    df_professionals,\n",
    "    ['professional_all_tags'],\n",
    "    'professionals_id_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset building for lightfm\n",
    "########################\n",
    "\n",
    "# define our dataset variable\n",
    "# then we feed unique professionals and questions ids\n",
    "# and item and professional feature list\n",
    "# this will create lightfm internel mapping\n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    set(df_professionals['professionals_id_num']), \n",
    "    set(df_questions['questions_id_num']),\n",
    "    item_features=question_feature_list, \n",
    "    user_features=professional_feature_list)\n",
    "\n",
    "\n",
    "# now we are building interactions matrix between professionals and quesitons\n",
    "# we are passing professional and questions id as a tuple\n",
    "# e.g -> pd.Series((pro_id, question_id), (pro_id, questin_id))\n",
    "# then we use lightfm build in method for building interactions matrix\n",
    "df_merge['author_question_id_tuple'] = list(zip(\n",
    "    df_merge.professionals_id_num, df_merge.questions_id_num, df_merge.total_weights))\n",
    "\n",
    "interactions, weights = dataset.build_interactions(\n",
    "    df_merge['author_question_id_tuple'])\n",
    "\n",
    "\n",
    "\n",
    "# now we are building our questions and professionals features\n",
    "# in a way that lightfm understand.\n",
    "# we are using lightfm build in method for building\n",
    "# questions and professionals features \n",
    "questions_features = dataset.build_item_features(\n",
    "    df_questions['question_features'])\n",
    "\n",
    "professional_features = dataset.build_user_features(\n",
    "    df_professionals['professional_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 1/5 [01:08<04:34, 68.64s/it]"
     ]
    }
   ],
   "source": [
    "# Model building part\n",
    "################################\n",
    "\n",
    "# define lightfm model by specifying hyper-parametre\n",
    "# then fit the model with ineteractions matrix, item and user features \n",
    "model = LightFM(\n",
    "    no_components=150,\n",
    "    learning_rate=0.05,\n",
    "    loss='warp',\n",
    "    random_state=2019)\n",
    "\n",
    "model.fit(\n",
    "    interactions,\n",
    "    item_features=questions_features,\n",
    "    user_features=professional_features, sample_weight=weights,\n",
    "    epochs=5, num_threads=4, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_auc_score(model, interactions, questions_features, professional_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n",
    "\n",
    "def recommend_questions(professional_ids):\n",
    "     \n",
    "    for professional in professional_ids:\n",
    "        # print their previous answered question title\n",
    "        previous_q_id_num = df_merge.loc[df_merge['professionals_id_num'] == professional][:3]['questions_id_num']\n",
    "        df_previous_questions = df_questions.loc[df_questions['questions_id_num'].isin(previous_q_id_num)]\n",
    "        print('Professional Id (' + str(professional) + \"): Previous Answered Questions\")\n",
    "        display_side_by_side(\n",
    "            df_previous_questions[['questions_title', 'question_features']],\n",
    "            df_professionals.loc[df_professionals.professionals_id_num == professional][['professionals_id_num','professionals_tag_name']])\n",
    "        \n",
    "        # predict\n",
    "        discard_qu_id = df_previous_questions['questions_id_num'].values.tolist()\n",
    "        df_use_for_prediction = df_questions.loc[~df_questions['questions_id_num'].isin(discard_qu_id)]\n",
    "        questions_id_for_predict = df_use_for_prediction['questions_id_num'].values.tolist()\n",
    "        \n",
    "        scores = model.predict(\n",
    "            professional,\n",
    "            questions_id_for_predict,\n",
    "            item_features=questions_features,\n",
    "            user_features=professional_features)\n",
    "        \n",
    "        df_use_for_prediction['scores'] = scores\n",
    "        df_use_for_prediction = df_use_for_prediction.sort_values(by='scores', ascending=False)[:8]\n",
    "        print('Professional Id (' + str(professional) + \"): Recommended Questions: \")\n",
    "        display(df_use_for_prediction[['questions_title', 'question_features']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_questions([1200 ,19897, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70437ad1e85be4a6249bd927ad4849ff63ae39d5847aa63d3658922017d6cc74"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
